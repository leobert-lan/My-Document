原文链接[这里](http://dataunion.org/11692.html)
## 什么是CNN
CNN的目的是用来处理图像识别，主要用来识别位移、缩放及其他形式扭曲不变性的二维图形。

卷积神经网络 Convolutional Neural Networks

20世纪60年代，Hubel和Wiesel在研究猫脑皮层中用于**局部敏感和方向选择**的神经元时发现其独特的网络结构可以有效地**降低反馈神经网络的复杂性**。

因为处理方式有点像信号处理中的卷积，所以叫卷积神经网络，本质还是神经网络。

## CNN能干什么
CNN主要用于**模式分类领域**。

该网络避免了对图像的复杂前期预处理，可以直接输入原始图像，因而得到了更为广泛的应用。

一般地，CNN的基本结构包括两层：

* 特征提取层：

每个神经元的输入与前一层的局部接受域相连，并提取该局部的特征。一旦该局部特征被提取后，它与其它特征间的位置关系也随之确定下来；

* 特征映射层：

网络的每个计算层由多个特征映射组成，每个特征映射是一个平面，平面上所有神经元的权值相等。

特征映射结构采用影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性。此外，由于**一个映射面上的神经元共享权值**，因而减少了网络自由参数的个数。

卷积神经网络中的每一个卷积层都紧跟着一个用来求局部平均与二次提取的计算层（池化层），这种特有的两次特征提取结构减小了特征分辨率

## 局部感知
可以降低参数数目，对应局部感知野。也就是，后层的神经元和前层的连接是有限的，局限在一个窗口内。而这种看起来很像卷积操作，可以认为是一种特征提取操作。

## 参数共享
哪怕使用了“卷积操作”，对于一个没有预处理的图片来说，可能参数也是庞大的，这时候再使用权值共享，可以使得参数数量大大降低。

在原文局部感知的例子中（这里没有图），`1000*1000px`的图，每个神经元只连接`10*10px`的窗口，依旧需要`1000000*100`个参数，（从全连接的例子出发，每个像素点对应一个神经元，局部感知只是减少连接数量，没有减少神经元）

权值共享的目的就是减少神经元。假设：图像的一部分的统计特性与其他部分是一样的，那么某一层中，用于某种特征提取的神经元就只需要一个！这时候只需要100个参数。但注意，这只能学习一种特征

## 多卷积核
就是多个卷积核，卷积核是什么呢？上面我们了解到，用一个神经元进行卷积操作，可以将一个输入图像进行卷积，输出成另一幅图像。抛开步长不谈。上面的例子中是将`10*10px`的窗口输出为一个像素。计算方法是做加权平均，而权值由一个函数定义，这个函数称为卷积核。可以简单的认为一个神经元对应了一个卷积核。

上面我们也提到了，一个神经元只能提取一种特征，这是不够充分的，往往使用多个卷积核提取不同的特征

## Down-pooling
原文中本小节计算有误，还用之前的例子，假设我们的窗口移动步长为10，那么单卷积核输出的“图像”是`（1000/10）*(1000/10)=10000`维度的特征，如果步长小一点那将更大。
如果是400核的，单样本输出400*10000维的卷积特征向量，这很可怕！而且容易过拟合。

而且，为了充分的提取特征，步长一般是比窗口宽高要小的，这样这个特征向量规模会更大，而且他对于原始图像的学习过于充分，这就是上面说的容易过拟合，鲁棒性降低了。

这时候就有了池化，也就是对FeatureMap进行采样，进行压缩。比如对原始的FeatureMap进行`2*2->1*1`的压缩采样，规模直接变为原来的25%。

采样往往采样取最大值或者取平均值。

采样带来的两大好处就是：

* 有效减少了特征向量的规模，减少了后层神经元数量
* 在微小范围内移动时，输出的特征一致，鲁棒性有所提升。


## 多层卷积
在实际应用中，往往使用多层卷积，然后再使用全连接层进行训练，多层卷积的目的是一层卷积学到的特征往往是局部的，层数越高，学到的特征就越全局化。

## 关于dropout
CNN容易过拟合，dropout是一种抑制过拟合的手段。

Dropout是在标准的bp网络的的结构上，使bp网的隐层激活值，以一定的比例v变为0，即按照一定比例v，随机地让一部分隐层节点失效；
